# ðŸ” Skill Quality Checker

A comprehensive tool to analyze **Claude AI skills** (created with Skill Seekers) and provide quality scores with auto-fix capabilities.

> **Note**: This tool analyzes skills generated by [Skill Seekers](https://github.com/yusufkaraaslan/skill-seekers). First create a skill, then use this tool to validate its quality.

## âœ¨ Features

### 1. ðŸ”— Link Validator
- âœ… Checks all URLs in SKILL.md and references/*.md files
- âœ… Detects broken links (404, timeout, SSL errors)
- âœ… Finds archive.org alternatives for broken links
- âœ… Reports link health percentage

### 2. ðŸ’» Code Block Validator
- âœ… Extracts all code blocks from markdown files
- âœ… Validates syntax for Python, JavaScript, TypeScript, JSON
- âœ… Reports code quality percentage
- âœ… Identifies syntax errors with file and line numbers

### 3. ðŸ“Š Content Analyzer
- âœ… Counts pages, tokens, code examples, images, links
- âœ… Calculates content density metrics
- âœ… Estimates reading time
- âœ… Provides comprehensive content statistics

### 4. ðŸ¤– AI Quality Score
- âœ… Uses Claude Sonnet 4 API to score skill quality
- âœ… Rates clarity, completeness, code quality, structure, usefulness
- âœ… Provides actionable recommendations
- âœ… Compares against quality benchmarks

### 5. ðŸ”§ Auto-Fix Engine
- âœ… Replaces broken links with archive.org versions
- âœ… Identifies formatting issues
- âœ… Applies fixes automatically with --auto-fix flag

## ðŸ“¦ Installation

```bash
# Clone this repository
git clone https://github.com/incarnator-x/skill-quality-checker.git
cd skill-quality-checker

# Install dependencies
pip install -r requirements.txt
```

## ðŸš€ Quick Start

### Step 1: Create a Skill with Skill Seekers

```bash
# First, use Skill Seekers to create a skill
cd /path/to/Skill_Seekers
python3 cli/doc_scraper.py --config configs/react.json

# Output: Skill_Seekers/output/react/
```

### Step 2: Check Quality

```bash
# Basic quality check (no API key needed)
python skill_quality_checker.py /path/to/Skill_Seekers/output/react/ --skip-ai

# Full check with AI scoring
export ANTHROPIC_API_KEY=sk-ant-...
python skill_quality_checker.py /path/to/Skill_Seekers/output/react/
```

## ðŸ“– Usage

### Basic Check
```bash
python skill_quality_checker.py /path/to/skill/directory/
```

### With Auto-Fix
```bash
python skill_quality_checker.py /path/to/skill/directory/ --auto-fix
```

### Generate Detailed Report
```bash
python skill_quality_checker.py /path/to/skill/directory/ --report quality_report.md
```

### Skip AI Scoring (for testing without API key)
```bash
python skill_quality_checker.py /path/to/skill/directory/ --skip-ai
```

## ðŸ“Š Example Output

```
======================================================================
ðŸ” Skill Quality Checker
======================================================================
ðŸ“ Skill: react
ðŸ“‚ Path: /path/to/Skill_Seekers/output/react
======================================================================

ðŸ”— [1/4] Validating links...
ðŸ“‚ Scanning /path/to/Skill_Seekers/output/react...
   Found 250 links in 45 files

ðŸ” Checking 250 unique URLs...
   Progress: 250/250
   âœ“ Completed in 12.3s

ðŸ’» [2/4] Validating code blocks...
ðŸ“ Found 120 code blocks
   âœ“ Completed in 3.2s

ðŸ“Š [3/4] Analyzing content...
   âœ“ Completed in 0.5s

ðŸ¤– [4/4] Getting AI quality score...
   âœ“ Completed in 8.7s

======================================================================
âœ… All checks completed in 24.7s
======================================================================

======================================================================
ðŸ“Š QUALITY SUMMARY
======================================================================

ðŸŽ¯ Overall Score: 8.5/10 â­â­â­â­

âœ… Link Health: 245/250 working (98.0%)
âœ… Code Quality: 115/120 valid (95.8%)

ðŸ“„ Content: 85 pages, 230,000 tokens, 120 examples
ðŸ¤– AI Score: 8.7/10

======================================================================
```

## ðŸ“ Report Format

When using `--report`, generates a comprehensive markdown report:

```markdown
# ðŸ“Š Skill Quality Report: React

## Summary
- **Overall Score**: 8.5/10 â­â­â­â­
- **Status**: âœ… Very Good
- **Generated**: 2025-11-09 23:45

## Link Health
âœ… **245/250** links working (98%)
âŒ Broken links (5):
  - https://old-docs.com/api â†’ 404
    âœ“ Archive: https://web.archive.org/...

## Code Quality
âœ… **115/120** code examples valid (95.8%)
âš ï¸ Issues (5):
  - hooks.md:45 - Missing import statement

## Content Analysis
- Total Pages: 85
- Total Tokens: 230,000
- Code Examples: 120

## AI Assessment
ðŸ¤– **Claude Score**: 8.7/10
- Clarity: 9/10
- Completeness: 8.5/10

## Recommendations
1. Fix 5 broken links
2. Add imports to 5 code examples
```

## âš™ï¸ Configuration

### AI Scoring (Optional)

Set your Anthropic API key to enable AI quality scoring:

```bash
# Windows
set ANTHROPIC_API_KEY=your_api_key_here

# Linux/Mac
export ANTHROPIC_API_KEY=your_api_key_here
```

**Without API key:** Use `--skip-ai` flag for basic validation (links, code, content).

### Code Validation

For JavaScript/TypeScript validation, install:

```bash
# Node.js (for JavaScript validation)
# Download from: https://nodejs.org/

# TypeScript (for TypeScript validation)
npm install -g typescript
```

## ðŸŽ¯ Exit Codes

The tool returns different exit codes for CI/CD integration:

- `0` - Excellent quality (score >= 8.0)
- `1` - Fair quality (score >= 6.0)
- `2` - Needs improvement (score < 6.0)

Example CI usage:

```bash
python skill_quality_checker.py output/react/ || echo "Quality check failed"
```

## ðŸ“ Project Structure

```
skill-quality-checker/
â”œâ”€â”€ skill_quality_checker.py  # Main CLI tool
â”œâ”€â”€ validators/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ link_validator.py     # Link validation
â”‚   â”œâ”€â”€ code_validator.py     # Code syntax validation
â”‚   â””â”€â”€ content_analyzer.py   # Content metrics
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ claude_api.py         # AI scoring with Claude
â”‚   â””â”€â”€ report_generator.py   # Markdown report generation
â”œâ”€â”€ examples/
â”‚   â””â”€â”€ sample_report.md      # Example report
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

## ðŸ§ª Testing

Test the tool with the included sample:

```bash
# Create a test skill directory
mkdir -p test_skill/references
echo "# Test" > test_skill/SKILL.md

# Run checker
python skill_quality_checker.py test_skill/ --skip-ai
```

## âš¡ Performance

| Task | Time | Notes |
|------|------|-------|
| Link Validation | 10-20s | Depends on number of links |
| Code Validation | 1-5s | Fast syntax checking |
| Content Analysis | <1s | Instant |
| AI Scoring | 10-30s | Depends on content size |
| **Total (with AI)** | **~30s** | For average skill (100 pages) |

## ðŸ¤ Workflow

1. **Create Skill**: Use [Skill Seekers](https://github.com/yusufkaraaslan/skill-seekers) to scrape documentation
2. **Check Quality**: Run this tool to validate the generated skill
3. **Fix Issues**: Use `--auto-fix` or manually fix reported issues
4. **Package**: Use Skill Seekers' `package_skill.py` to create .zip
5. **Upload**: Upload to Claude

## ðŸ› Troubleshooting

### "ANTHROPIC_API_KEY not found"
**Solution**: Either set the environment variable or use `--skip-ai` flag.

### "Node.js not installed"
**Solution**: JavaScript/TypeScript blocks will be skipped. Install Node.js for full validation.

### Slow link validation
**Solution**: The tool uses parallel requests (10 concurrent). For very large skills, this is normal.

## ðŸ“š Related Projects

- **[Skill Seekers](https://github.com/yusufkaraaslan/skill-seekers)** - Create Claude AI skills from any documentation

## ðŸ“„ License

MIT License - See LICENSE file for details

## ðŸ¤– About

This tool was designed to work with skills generated by [Skill Seekers](https://github.com/yusufkaraaslan/skill-seekers), ensuring high-quality Claude AI skills before upload.

## ðŸ™ Contributing

Contributions welcome! Please open an issue or PR.

---

**Made with â¤ï¸ for the Claude AI community**
